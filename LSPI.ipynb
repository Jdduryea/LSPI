{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# This is a notebook for testing my implementation of LSPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40485638,  0.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "#env = gym.make(\"Pendulum-v0\")\n",
    "#env = gym.make(\"NChain-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09762701])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08734086,  0.01438687])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSPI(basis_functions, gamma, epsilon, w, env, method = \"discrete\"):\n",
    "    '''\n",
    "    Compute the parameters of the policy, w, using the LSPI algorithm.\n",
    "    \n",
    "    Inputs:\n",
    "    sample: list of tuples of the form (s,a,r,s')\n",
    "    basis_functions: list of basis functions\n",
    "    gamma: float, discount factor\n",
    "    epsilon: float, convergence threshold\n",
    "    w: intial policy parameter vector\n",
    "    \n",
    "    Outputs:\n",
    "    w: the converged policy paramters\n",
    "    '''\n",
    "    w0 = []\n",
    "    samples = generate_samples(env, 10000, 20)\n",
    "    #while True:\n",
    "    for i in tqdm(range(100)):\n",
    "        w_prev = w\n",
    "        #w = LSTDQ(samples, basis_functions, gamma, w, env)\n",
    "        w = LSTDQ_OPT(samples, basis_functions, gamma, w, env, method = method)\n",
    "        \n",
    "        \n",
    "        if converged(w, w_prev, epsilon):\n",
    "            break \n",
    "        else:\n",
    "            w_prev = w\n",
    "        w0.append(w[0])\n",
    "        print w[0]\n",
    "    return w, w0\n",
    "\n",
    "def converged(w, w_prev, epsilon):\n",
    "    '''\n",
    "    Determines if the policy parameters have converged based\n",
    "    on whether or not the norm of the difference of w\n",
    "    is less than the threshold epsilon.\n",
    "    \n",
    "    Inputs:\n",
    "    w: a policy parameter vector\n",
    "    w_prev: the policy parameter vetor from a previous iteration.\n",
    "    epsilon: float, convergence threshold\n",
    "    '''\n",
    "    return np.linalg.norm(w - w_prev) < epsilon\n",
    "\n",
    "def LSTDQ(samples, basis_functions, gamma, w, env, method=\"discrete\"):\n",
    "    '''\n",
    "    Simple version of LSTDQ\n",
    "    '''\n",
    "    k = len(basis_functions)\n",
    "    #A = np.zeros((k,k)), this might not have an inverse, use the next line instead\n",
    "    A = np.identity(k) * 0.01\n",
    "    b = np.zeros(k)\n",
    "    \n",
    "    #samples[np.random.choice(len(samples), 100, replace=False)]\n",
    "    \n",
    "    for s, a, r, sp in samples:\n",
    "        phi = compute_phi(s,a, basis_functions)\n",
    "        phi_p = compute_phi(sp, get_policy_action(sp, w, basis_functions, env, method), basis_functions)\n",
    "\n",
    "        A += np.outer(phi, (phi - gamma*phi_p))\n",
    "        b = b + phi*r\n",
    "    \n",
    "    \n",
    "    w = np.dot(np.linalg.inv(A),b)\n",
    "    return w\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def LSTDQ_OPT(samples, basis_functions, gamma, w, env, sigma=0.1, method = \"discrete\" ):\n",
    "    '''\n",
    "    A faster version of LSTDQ. Computes an approximation of the policy parameters based\n",
    "    on the LSTDQ-OPT algorithm presented in the paper.\n",
    "    \n",
    "    Inputs:\n",
    "    sample: list of tuples of the form (s,a,r,s')\n",
    "    basis_functions: list of basis functions\n",
    "    gamma: float, discount factor\n",
    "    epsilon: float, convergence threshold\n",
    "    w: intial policy parameter vector\n",
    "    \n",
    "    sigma: small positive float.\n",
    "    '''\n",
    "    k = len(basis_functions)\n",
    "    B = np.identity(k) * float(1/sigma)\n",
    "    b = np.zeros(k)\n",
    "    \n",
    "    for s, a, r, sp in samples:\n",
    "        phi = compute_phi(s,a, basis_functions)\n",
    "        phi_p = compute_phi(sp, get_policy_action(sp, w, basis_functions, env, method), basis_functions)\n",
    "\n",
    "        # Some computations that can be reused in the computation\n",
    "        Bphi = np.dot(B, phi)\n",
    "        phi_t =  (phi - gamma*phi_p).T\n",
    "        \n",
    "\n",
    "        top = np.dot(np.outer(Bphi, phi_t), B)\n",
    "        bottom = 1 + np.dot(phi_t, Bphi)\n",
    "        B = B - top/bottom\n",
    "        \n",
    "        b = b + phi*r\n",
    "    \n",
    "    w = np.dot(B,b)\n",
    "        \n",
    "        \n",
    "    return w\n",
    "       \n",
    "\n",
    "def compute_phi(s,a, basis_functions):\n",
    "    '''\n",
    "    Computes the vector ϕ(s,a) according to the basis function ϕ_1...ϕ_k\n",
    "    \n",
    "    Inputs:\n",
    "    s: state\n",
    "    a: action\n",
    "    basis_functions: list of basis functions that operate on s and a\n",
    "    \n",
    "    Outputs:\n",
    "    ϕ(s,a), a vector where each entry is the result of one of the basis functions.\n",
    "    '''\n",
    "\n",
    "    phi= np.array([bf(s,a) for bf in basis_functions])\n",
    "    return phi\n",
    "    \n",
    "    \n",
    "def get_policy_action(s,w, basis_functions, env, method = \"discrete\"):\n",
    "    if method == \"discrete\":\n",
    "        return get_policy_action_discrete(s,w,basis_functions,env)\n",
    "    if method == \"continuous\":\n",
    "        return get_policy_actions_continuous_discretized(s, w, basis_functions, env)\n",
    "    \n",
    "def get_policy_action_discrete(s, w, basis_functions, env):\n",
    "    '''\n",
    "    For discrete action spaces. Given a parameterization for the policy,\n",
    "    reconstruct the policy and querery it to get \n",
    "    the optimal action for state s. That is,\n",
    "    the argmax over actions of ϕ(s,a).w\n",
    "    \n",
    "    Inputs:\n",
    "    s: state\n",
    "    w: policy parameters\n",
    "    basis_functions: the basis functions that are used in the model\n",
    "    \n",
    "    Outputs:\n",
    "    action a that the policy says is best\n",
    "    '''\n",
    "    a_max = None\n",
    "    max_score = float(\"-inf\")\n",
    "    \n",
    "    action_space = [0,1]\n",
    "    # Search action space for most valuable action\n",
    " \n",
    "    #TODO:  use sympy for grad desc\n",
    "    for a in action_space:\n",
    "        score = np.dot(compute_phi(s,a, basis_functions), w)\n",
    "        # update if we found something better\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            a_max = a\n",
    "    return a_max    \n",
    "    \n",
    "    \n",
    "def get_policy_actions_continuous_discretized(s, w, basis_functions, env, n_discretizations=10):\n",
    "    '''\n",
    "    For discrete action spaces. Given a parameterization for the policy,\n",
    "    reconstruct the policy and querery it to get \n",
    "    the optimal action for state s. That is,\n",
    "    the argmax over actions of ϕ(s,a).w\n",
    "    \n",
    "    Inputs:\n",
    "    s: stateget_policy_action\n",
    "    w: policy parameters\n",
    "    basis_functions: the basis functions that are used in the model\n",
    "    n_discretizations: the number of chunks to split the continuous space into\n",
    "       \n",
    "    \n",
    "    Outputs:\n",
    "    action a that the policy says is best\n",
    "    '''\n",
    "    \n",
    "    a_max = None\n",
    "    max_score = float(\"-inf\")\n",
    "    \n",
    "    # Discretize the continuous space into n_discretizations chunks\n",
    "    action_space = np.linspace(env.action_space.low[0], env.action_space.high[0], n_discretizations)\n",
    "    for a in action_space:\n",
    "        score = np.dot(compute_phi(s, a, basis_functions), w)\n",
    "        # update if we found something better\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            a_max = a\n",
    "            \n",
    "    return a_max\n",
    "\n",
    "def get_cartpole_basis_functions_v1():\n",
    "    bf1 = lambda s,a:a\n",
    "    bf2 = lambda s,a:s[0]\n",
    "    bf3 = lambda s,a:s[1]\n",
    "    bf4 = lambda s,a:s[2]\n",
    "    bf5 = lambda s,a:s[3]\n",
    "    bfs = [bf1,bf2,bf3,bf4,bf5]\n",
    "    return bfs\n",
    "   \n",
    "def get_cartpole_basis_functions_v2():\n",
    "    '''\n",
    "    Returns a list of basis functions that seem \n",
    "    to work well for the simplified (no singularity, starting above the horizon)\n",
    "    cartpole problem.\n",
    "    \n",
    "    Note: this one seems to work the best so far.\n",
    "    '''\n",
    "    s1 = np.array([1,1,1,1])\n",
    "    s2 = np.array([0,0,0,0])\n",
    "    s3 = np.array([1,0,1,0])\n",
    "\n",
    "    bf1 = lambda s,a: 1\n",
    "\n",
    "    bf2 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s1)/2.0)\n",
    "    bf3 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s1)/2.0)\n",
    "\n",
    "    bf4 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s2)/2.0)\n",
    "    bf5 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s2)/2.0)\n",
    "\n",
    "    bf6 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s3)/2.0)\n",
    "    bf7 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s3)/2.0)\n",
    "    \n",
    "    \n",
    "    bfs = [bf1,bf2, bf3, bf4, bf5, bf6, bf7]\n",
    "    \n",
    "    return bfs\n",
    "\n",
    "\n",
    "def get_cartpole_basis_functions_v3():\n",
    "\n",
    "    s1 = np.array([-1,-1,0,0])\n",
    "    s2 = np.array([-0.5,-1,0,0])\n",
    "    s3 = np.array([-0.1,-1,0,-0.5])\n",
    "    s4 = np.array([0,0,0,0])\n",
    "    s5 = np.array([0.1,1,0,0])\n",
    "    s6 = np.array([0.5,0.5,0,-0.5])\n",
    "    s7 = np.array([1,0,0,0])\n",
    "\n",
    "    bf1 = lambda s,a: 1\n",
    "\n",
    "    bf2 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s1)/2.0)\n",
    "    bf3 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s1)/2.0)\n",
    "\n",
    "    bf4 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s2)/2.0)\n",
    "    bf5 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s2)/2.0)\n",
    "\n",
    "    bf6 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s3)/2.0)\n",
    "    bf7 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s3)/2.0)\n",
    "    \n",
    "    bf8 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s4)/2.0)\n",
    "    bf9 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s4)/2.0)\n",
    "    \n",
    "    bf10 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s5)/2.0)\n",
    "    bf11 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s5)/2.0)\n",
    "    \n",
    "    bf12 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s6)/2.0)\n",
    "    bf13 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s6)/2.0)\n",
    "    \n",
    "    bf14 = lambda s,a: int(a==0)*np.exp( - np.linalg.norm(s-s7)/2.0)\n",
    "    bf15 = lambda s,a: int(a==1)*np.exp( - np.linalg.norm(s-s7)/2.0)\n",
    "\n",
    "    bfs = [bf1,bf2,bf3,bf4, bf5, bf6, bf7, bf8, bf9, bf10, bf11, bf12, bf13, bf14, bf15]\n",
    "    return bfs\n",
    "    \n",
    "    \n",
    "def get_cartpole_basis_functions_v4():\n",
    "    bf1 = lambda s,a: 1\n",
    "    bf2 = lambda s,a: a\n",
    "    bf3 = lambda s,a: int(a==0)*s[0]\n",
    "    bf4 = lambda s,a: int(a==0)*s[1]\n",
    "    bf5 = lambda s,a: int(a==0)*s[2]\n",
    "    bf6 = lambda s,a: int(a==0)*s[3]\n",
    "    bf7 = lambda s,a: int(a==1)*s[0]\n",
    "    bf8 = lambda s,a: int(a==1)*s[1]\n",
    "    bf9 = lambda s,a: int(a==1)*s[2]\n",
    "    bf10 = lambda s,a: int(a==1)*s[3]\n",
    "    return [bf1,bf2,bf3,bf4,bf5, bf6, bf7, bf8, bf8, bf10]\n",
    "\n",
    "    \n",
    "def get_continuous_mt_car_basis_functions():\n",
    "    '''\n",
    "    Define some basis functions and return them in a list\n",
    "    '''\n",
    "    bfs = []\n",
    "\n",
    "    bf1 = lambda s,a: 1\n",
    "    bf2 = lambda s,a: s[0]\n",
    "    bf3 = lambda s,a: a\n",
    "    bf4 = lambda s,a: a*s[0]*s[0]\n",
    "    bf5 = lambda s,a: a*s[0]*s[0]*s[0]\n",
    "    bf6 = lambda s,a: a*s[1]\n",
    "    bf7 = lambda s,a: a*s[1]*s[1]\n",
    "    bf8 = lambda s,a: a*s[1]*s[1]*s[1]\n",
    "    bf9 = lambda s,a: a*s[0]*s[1]\n",
    "    bf10 = lambda s,a: a*s[0]*s[0]*s[1]*s[1]\n",
    "\n",
    "\n",
    "    bfs = [bf1,bf2,bf3,bf4,bf5, bf6,bf7,bf8,bf9,bf10]\n",
    "    return bfs\n",
    "\n",
    "\n",
    "def generate_samples(env, n_samples, n_steps=100):\n",
    "    samples = []\n",
    "    print env.reset()\n",
    "    for i in range(n_samples):\n",
    "        env.reset()\n",
    "        for j in range(n_steps):\n",
    "            s = list(env.env.state)\n",
    "\n",
    "            a = env.action_space.sample()\n",
    "            #print \"A1:\",a\n",
    "            #a = np.random.choice(np.linspace(-1,1,10))\n",
    "            #print \"A2:\",a\n",
    "            #a = np.random.choice(np.linspace(0,1,2))\n",
    "            sp,r, _,_ = env.step(a)\n",
    "            \n",
    "            sample = (s, a, r, sp)\n",
    "            samples.append(sample)\n",
    "\n",
    "    return np.array(samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6   0.07]\n",
      "[-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print env.observation_space.high\n",
    "print env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print env.action_space.sample()\n",
    "print env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53432865  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 1/100 [00:28<47:00, 28.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0352926882954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 2/100 [00:52<44:22, 27.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0354917784251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.54917773e-02  -9.00777851e-04  -8.03927337e-04   8.64635125e-04\n",
      "  -5.61739579e-03   4.17304863e-02  -8.19483137e-05  -1.57914767e-05\n",
      "  -7.96584426e-03   6.33860122e-05]\n"
     ]
    }
   ],
   "source": [
    "bfs = get_basis_functions()\n",
    "\n",
    "\n",
    "gamma, epsilon, k = 0.05, 0.0001, len(bfs)\n",
    "w = np.zeros(k)\n",
    "w_est, w0 = LSPI(bfs, gamma, epsilon, w, env, method = \"continuous\")\n",
    "print w_est\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11261fe50>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAD8CAYAAACo9anUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VdW5x/HvLwmDgGCQQJnDEFREZIgoAkFbRrXEuSgq\nzqIiSDpIr/dqK7VFbQEHFMeKFUXEAVQEAtUEEJSAgMyTKCBDBARRBJH3/nE2vcfcgwkeck6G9/M8\n5zlnr7PW3mtJmx97Z/NumRnOOedcLCXEewLOOefKHw8f55xzMefh45xzLuY8fJxzzsWch49zzrmY\n8/BxzjkXcx4+zjnnYs7DxznnXMx5+DjnnIu5pHhPoKSqVauWpaamxnsazjlXqixYsOBLM0sprJ+H\nzxGkpqaSl5cX72k451ypIumzovTzy27OOedizsPHOedczHn4OOecizkPH+ecczHn4eOccy7mPHyc\nc87FnIePc865mPPwOcbMjL9OWcH6/L3xnopzzpVYHj7H2KdffsP4jz6n98OzGJOzjoM/HIr3lJxz\nrsTx8DnGmqZUIzurK11bpDD83ZVc+Pgcln+xJ97Tcs65EsXDpxjUqV6ZJ69uz+P92rF193f0eWw2\n/5i+iv0Hf4j31JxzrkTw8CkmkjjvtLpkD+lKnzb1ePTfaznv4Vks+GxnvKfmnHNxF1X4SKopKVvS\nmuA9+Qj9eklaJWmtpKFh7cMkLZG0SNJ0SfWC9lRJ+4L2RZLGhI2ZKmmxpGWSxkhKDNorSXolOMaH\nklLDxvQP5rhGUv9o1ny0kqtWZMTlbXj+ujP47vtDXDpmLn+avIxv9h+M5TScc65EifbMZygw08zS\ngJnB9o8E4TAa6A20BK6Q1DL4+iEza21mbYC3gXvChq4zszbBa0BY++VmdjrQCkgBLgvabwB2mVlz\nYCTwQHD8msC9wJlAB+DeI4VkcTrnpNpMG5LB1Wc15vkPNtBzVC6z1uTHehrOOVciRBs+mcDY4PNY\n4MIIfToAa81svZkdAMYH4zCz8N/EVwWssAOGjUkCKoaNCZ/LROBXkgT0BLLNbKeZ7QKygV5FW96x\nVa1SEvdltmLCLR2pmJjA1c9+xO9fXczub7+Px3Sccy5uog2fOma2Jfi8FagToU99YGPY9qagDQBJ\n90vaCPTjx2c+TYJLbjmSuoTvUNI0YDvwNaGg+dFxzOwgsBs4sbDjx0OHJjWZMrgLt53TjNc/3ky3\nkTlMXbo1nlNyzrmYKjR8JM2QtDTCKzO8n5kZRThzKcjM7jazhsA4YGDQvAVoFFyOywJeklQ9bExP\noC5QCfjl0R7zSCTdLClPUl5+fvFeEqtcIZE/9DqZSbd3IqVaJQa8uIDbxi1g+9ffFetxnXOuJCg0\nfMysm5m1ivCaBGyTVBcgeN8eYRebgYZh2w2CtoLGAZcEx9xvZjuCzwuAdUCLAvP6DphEcAkv/DiS\nkoAawI6jOD5m9pSZpZtZekpKoU+BPSZa1a/BpIGd+H3Pk5ixYjvdR+QyccEmQlnunHNlU7SX3SYD\nh+8e608oDAqaD6RJaiKpItA3GIektLB+mcDKoD0l7C62pkAasF5StbCwSwLOPzymwFwuBf4dnI1N\nA3pISg5uNOgRtJUYFRITuP3c5kwZ1IXmtavxu1cX0/+f89m069t4T80554pFtOEzHOguaQ3QLdhG\nUj1JU+A/v38ZSOgH/gpggpktOzw+uIS3hFAoDA7aM4AlkhYR+p3OADPbSeimhMlB/0WEzrQO34b9\nLHCipLWELtUNDY6/ExhGKATnA/cFbSVO89rVePWWjvy5z6nkbdhJj5G5jP1gA4cO+VmQc65skV/e\niSw9Pd3y8vLidvxNu77lv95YSu7qfNIbJ/PApa1pllItbvNxzrmikLTAzNIL6+cVDkqoBslVGHvd\nGfz9stNZs30vvR+exej31vK9Fyp1zpUBHj4lmCQubd+A7KwMup1Sm4emrSLzsTks3bw73lNzzrmo\nePiUArWPr8zj/doz5qp2bP96P5mj5/DA1JV8970XKnXOlU4ePqVIr1Z1mZnVlYvb1ueJ99dx3sOz\nmL+hRN474ZxzP8nDp5SpUaUCD112Oi9c34H9Bw9x2Zi53DNpKXu9UKlzrhTx8CmlMlqkMH1IBtee\nncq/5n1Gz5G55Kz2QqXOudLBw6cUq1opiT/1OZWJAzpSuUIC/Z/7iKwJi9j1zYF4T805536Sh08Z\n0L5xTd4Z1IWB5zZn8qIv6D4yhymfbPESPc65EsvDp4yoXCGR3/U8iUkDO/GLGpW5bdxCBry4gO17\nvFCpc67k8fApY06tV4M3b+vEXb1O5r1V+XQbkcOEvI1+FuScK1E8fMqgpMQEbj2nGVMHd+HkX1Tn\nDxOXcPWzH7Fxpxcqdc6VDB4+ZVjTlGqMv/kshl3Yio8/30WPkbn8c86n/OCFSp1zcebhU8YlJIir\nz2rM9KyunNm0Jn9+azmXjfmANdu+jvfUnHPlmIdPOVH/hOP457VnMPI3p7P+y284/5HZPDpzjRcq\ndc7FhYdPOSKJi9o2YEZWV7qfWod/ZK/m14/O5pNNXqjUORdbHj7lUK1qlRh9ZTuevLo9O785QObo\n2fzt3RVeqNQ5FzNRhY+kmpKyJa0J3pOP0K+XpFWS1koaGtY+TNISSYskTZdUL2hPlbQvaF8kaUzY\nmKmSFktaJmlM2OO2syQtD/Y3U1LjsDE/hO1rcjRrLkt6nvoLsrO6cnl6Q57MWU/vh2fx4fod8Z6W\nc64ciOpJppIeBHaa2fAgVJLN7K4CfRKB1UB3YBOhR1lfYWbLJVU3sz1Bv0FASzMbICkVeNvMWkU4\nZnUz2yNJhB6x/aqZjZd0LvChmX0r6VbgHDP7TTBmr5kd1WNA4/0k01ibs/ZLhr6+hI0793HVWY24\nq9fJHF+5Qryn5ZwrZWL1JNNMYGzweSxwYYQ+HYC1ZrbezA4A44NxHA6eQFWg0CQMG5MEVDw8xsze\nM7PD/5BlHtDg6JZSvnVqXotpd2ZwQ+cmjPvwc3qMzOW9ldvjPS3nXBkVbfjUMbMtweetQJ0IfeoD\nG8O2NwVtAEi6X9JGoB9wT1i/JsFlshxJXcJ3KGkasB34mtDZT0E3AO+GbVeWtFDSPEmRAvLwfm+W\nlCcpLz+//FWIrlIxif+5oCWv3Xo21Solcd3z87lz/Mfs9EKlzrljrNDwkTRD0tIIr8zwfha6fnfU\n1/DM7G4zawiMAwYGzVuARmbWBsgCXpJUPWxMT6AuUAn4ZYH5XgWkAw+FNTc2s3bAlcAoSc2OMJen\nzCzdzNJTUlKOdillRrtGybw9qDODfpXG20u20H1EDm8t/sJL9DjnjplCw8fMuplZqwivScA2SXUB\ngvdI12k2Aw3DthsEbQWNAy4JjrnfzHYEnxcA64AWBeb1HTCJ4BJeMIduwN1AHzPbH9Z3c/C+Hngf\naFvYusu7SkmJZHVvwVt3dKZ+8nHc8fLH3PTCArZ5oVLn3DEQ7WW3yUD/4HN/QmFQ0HwgTVITSRWB\nvsE4JKWF9csEVgbtKWF3sTUF0oD1kqqFhV0ScH7YmLbAk4SC5z8hKClZUqXgcy2gE7A8ynWXG6fU\nrc7rt57N3eedwqw1oUKl4z/63M+CnHNRiTZ8hgPdJa0BugXbSKonaQqAmR0kdDltGrACmGBmyw6P\nDy7hLQF6AIOD9gxgiaRFhH6nM8DMdhK6KWFy0H8RoTOtw7dhPwRUA14tcEv1KUCepMXAe8BwM/Pw\nOQpJiQnclNGUaXdm0LJudYa+/gn9nvmQz3Z8E++pOedKqahutS7Lytut1kV16JAxfv5G/jplBQcP\nHeJ3PU7iuk5NSExQvKfmnCsBYnWrtStnEhLElWc2Ijsrg7Ob1eIv76zg4ic+YNVWL1TqnCs6Dx/3\ns9StcRzP9k/n4b5t2LjzWy54dBajZqzmwEEvVOqcK5yHj/vZJJHZpj7ZQzI477S6jJqxhl8/OpvF\nG7+K99SccyWch4+L2onVKvFw37Y8c006u/d9z0WPz+H+d5az74AXKnXORebh446Zbi3rMD0rg74d\nGvH0rE/p9XAuH6z7Mt7Tcs6VQB4+7piqXrkCf73oNF666UwArnz6Q/74+ifs+e77OM/MOVeSePi4\nYnF2s1pMHZzBzRlNeWX+53QfkcOM5dviPS3nXAnh4eOKzXEVE/mv807hjds6kVylIje+kMeglz9m\nx979hQ92zpVpHj6u2J3e8AQmD+zMkG4teHfpFrqNyGHSos1eose5cszDx8VExaQEBndL451BXWh8\nYlUGj1/EjWPz2LJ7X7yn5pyLAw8fF1Mt6hzPa7eezX+ffwpz1n1J9xG5jPvwMw4d8rMg58oTDx8X\nc4kJ4sYuTZl+Z1daN6jB3W8s5Yqn5/Hpl16o1LnywsPHxU2jE6sw7sYzGX7xaSz/Yg+9RuXyVO46\nDv7gJXqcK+s8fFxcSaJvh0ZkZ3WlS1oKf52ykouf+IAVW/bEe2rOuWLk4eNKhF/UqMzT17TnsSvb\nsnnXPn796GxGZK9m/0Ev0eNcWRRV+EiqKSlb0prgPfkI/XpJWiVpraShYe3DJC0JHv42XVK9oD1V\n0r6gfZGkMWFjpkpaLGmZpDFhTzy9VlJ+2Jgbw8b0D+a4RlJ/XIkkiQta12NGVld+fXo9Hpm5hgse\nmc3Cz3fFe2rOuWMsqofJSXoQ2Glmw4NQSTazuwr0SQRWA92BTYQeq32FmS2XVN3M9gT9BgEtzWyA\npFTgbTNrFeGY1c1sjyQResrpq2Y2XtK1QLqZDSzQvyaQB6QDBiwA2pvZT/5E84fJxd97K7fzX298\nwtY933F9pyb8tkcLqlRMive0nHM/IVYPk8sExgafxwIXRujTAVhrZuvN7AAwPhjH4eAJVCUUDj8p\nbEwSULEIY3oC2Wa2MwicbKBXYcdx8XfuybWZPiSDfmc24tnZn9JzVC5z1nqhUufKgmjDp46ZbQk+\nbwXqROhTH9gYtr0paANA0v2SNgL9gHvC+jUJLp/lSOoSvkNJ04DtwNeEzn4Ou0TSJ5ImSmpYlOO7\nku34yhX4y4Wn8crNZ5GUkEC/Zz7krolL2L3PC5U6V5oVGj6SZkhaGuGVGd7PQtfvjvoanpndbWYN\ngXHA4UtmW4BGZtYGyAJeklQ9bExPoC5QCfhl0PwWkGpmpxE6uzl8RlZkkm6WlCcpLz8//2iHu2J0\nZtMTeXdwFwZ0bcbEhZvoPiKH6cu2xntazrmfqdDwMbNuZtYqwmsSsE1SXYDgfXuEXWwGGoZtNwja\nChoHXBIcc7+Z7Qg+LwDWAS0KzOs7YBL/dwlvh5kdrlj5DND+KI+PmT1lZulmlp6SkhKpi4ujyhUS\nGdr7ZN68rRMnVqvEzf9awO0vLST/ay9U6lxpE+1lt8nA4bvH+hMKg4LmA2mSmkiqCPQNxiEpLaxf\nJrAyaE8Ju4utKZAGrJdULSzskoDzw8bUDdtXH2BF8Hka0ENScnA3Xo+gzZVSpzWoweSBnfhdjxZk\nL9tG95E5vL5wkxcqda4UifbWoeHABEk3AJ8BlwMEt0w/Y2bnmdlBSQMJ/cBPBJ4zs2WHx0s6CTgU\njB8QtGcA90n6PvhugJntlFQHmCypEqHgfA84fBv2IEl9gIPATuBagGDcMEIhCHCfme2Mct0uziok\nJjDwl2n0avUL/jBxCVkTFjN58Rfcf9Fp1D/huHhPzzlXiKhutS7L/Fbr0uOHQ8YLczfw4NRVJAiG\n9j6Zfmc2JiFB8Z6ac+VOrG61di7uEhPEdZ2aMH1IBu0aJ/M/k5bR96l5rM/fG++pOeeOwMPHlRkN\na1bhhes78NClrVm5dQ+9Hp7FE+97oVLnSiIPH1emSOKy9IbMyOrKuSel8MDUlVz4+ByWf+GFSp0r\nSTx8XJlUu3plnrw6nSf6tWPr7v30eWw2f5+2iu++90KlzpUEHj6uTOt9Wl1mZGWQ2aY+j723lvMf\nmcWCz/xmR+fizcPHlXknVKnIPy4/nbHXd+C77w9x6Zi5/GnyMr7ZfzDeU3Ou3PLwceVG1xYpTBuS\nwTVnNWbs3A30GJlL7movo+RcPHj4uHKlWqUk/pzZigm3dKRShQSuee4jfvfqYnZ/64VKnYslDx9X\nLp2RWpMpg7pw2znNeOPjzXQbmcPUpVsKH+icOyY8fFy5VblCIn/odTKTbu9ESrVKDHhxIbe+uIDt\nX38X76k5V+Z5+Lhyr1X9Gkwa2Inf9zyJmSu3031ELq/mbfRCpc4VIw8f5wgVKr393OZMGdSFtNrV\n+P3EJVzz3Eds3PltvKfmXJnk4eNcmOa1qzHhlo7cl3kqCz/bRc9RuTw/51MOHfKzIOeOJQ8f5wpI\nSBDXdExl2pAM0lNr8qe3lnP5k3NZu90LlTp3rHj4OHcEDZKrMPa6M/jHZaezZvteznt4FqPfW8v3\nXqjUuah5+Dj3EyRxSfsGzMjqSreWtXlo2ioyH5vD0s274z0150q1qMJHUk1J2ZLWBO/JR+jXS9Iq\nSWslDQ1rHyZpiaRFkqYHT0BFUqqkfUH7IkljwsZMlbRY0jJJY8Ietz0yrP9qSV+Fjfkh7LvJ0azZ\nlU8px1fi8X7tGXNVO/L37idz9BwemLrSC5U69zNF9SRTSQ8CO81seBAqyWZ2V4E+icBqoDuwidDj\nrK8ws+WSqpvZnqDfIKClmQ2QlAq8bWatIhyzupntkSRgIvCqmY0v0OcOoK2ZXR9s7zWzakezNn+S\nqTuS3d9+z/1TljMhbxNNa1XlgUtbc0ZqzXhPy7kSIVZPMs0ExgafxwIXRujTAVhrZuvN7AAwPhjH\n4eAJVAUKTcKwMUlAxSOMuQJ4uSgLcO5o1ahSgQcvPZ0XbziTAz8c4rIxc7ln0lL2eqFS54os2vCp\nY2aHa5JsBepE6FMf2Bi2vSloA0DS/ZI2Av2Ae8L6NQkuk+VI6hK+Q0nTgO3A14TOfsK/aww0Af4d\n1lxZ0kJJ8yRFCkjnjlrntFpMuzOD6zql8q95n9FzZC7vr9oe72k5VyoUGj6SZkhaGuGVGd7PQtfv\njvoanpndbWYNgXHAwKB5C9DIzNoAWcBLkqqHjekJ1AUqAb8ssMu+wEQzC78Y39jM2gFXAqMkNTvC\nWm+WlCcpLz/fqx27wlWtlMS9vz6ViQPO5riKiVz7z/lkTVjErm8OxHtqzpVohYaPmXUzs1YRXpOA\nbZLqAgTvkf7atxloGLbdIGgraBxwSXDM/Wa2I/i8AFgHtCgwr++ASQSX8ML0pcAlNzPbHLyvB94H\n2h5hrU+ZWbqZpaekpETq4lxE7Rsn886gztzxy+ZMXvQF3Ufm8M6SLV6ix7kjiPay22Sgf/C5P6Ew\nKGg+kCapiaSKhMJhMoCktLB+mcDKoD0l7C62pkAasF5StbCwSwLOPzwmaDsZSAbmhrUlS6oUfK4F\ndAKWR7lu5/6fSkmJ/LbHSUwe2Jm6NY7j9pcWcsu/FrB9jxcqda6gaMNnONBd0hqgW7CNpHqSpgCY\n2UFCl9OmASuACWa27PD44BLeEqAHMDhozwCWSFpE6Hc6A8xsJ6GbEiYH/RcROtP6z23YhIJtvP34\nr5unAHmSFgPvAcPNzMPHFZuW9arzxm1n88feJ5OzOp9fjchhwnwvVOpcuKhutS7L/FZrdyysz9/L\n0Nc/4aNPd9K5eS3+dvFpNKxZJd7Tcq7YxOpWa+fcT2iaUo3xN53FXy5sxaKNX9FjZC7Pzf6UH7xQ\nqSvnPHycK2YJCeKqsxozfUgGZzatyX1vL+fSMR+wZtvX8Z6ac3Hj4eNcjNQ74Tj+ee0ZjPpNGzZ8\n+Q3nPzKbR2au4cBBL1Tqyh8PH+diSBIXtq1PdlZXerb6BSOyV9Pnsdks2fRV4YOdK0M8fJyLg1rV\nKvHoFW15+pp0dn17gAtHz+FvU1Z4oVJXbnj4OBdH3VvWYfqQrvzmjIY8mbueXqNymbd+R7yn5Vyx\n8/BxLs5qHFeBv13cmpduPJNDBn2fmsfdb3zC1999H++pOVdsPHycKyHObl6LqXd24cbOTXj5o8/p\nMTKXf6/cFu9pOVcsPHycK0GqVEzivy9oyWu3nk21Sklc/3wed47/mJ1eqNSVMR4+zpVAbRsl8/ag\nzgz+VRrvfLKFbiNymLz4Cy/R48oMDx/nSqhKSYkM6d6Ct+7oTMPk4xj08sfc9MICtu72QqWu9PPw\nca6EO/kX1Xn9tk7cfd4pzF6bT/cRObz80ed+FuRKNQ8f50qBxARxU0ZTpg7O4NT61fnj659w5dMf\n8tmOb+I9Ned+Fg8f50qR1FpVeenGs/jrRaexdPNueo7K5ZlZ671QqSt1PHycK2USEsSVZzZielYG\nnZrV4i/vrODiJz5g1VYvVOpKDw8f50qpujWO45n+6TxyRVs27vyWCx6dxagZq71QqSsVogofSTUl\nZUtaE7wnH6FfL0mrJK2VNDSsfZikJZIWSZouqV7QnippX9C+SNKYCPucLGlp2HYlSa8Ex/hQUmrY\nd/2DOa6R1L/gvpwrrSTR5/R6zMjqynmn1WXUjDX8+tHZLNrohUpdyRbtmc9QYKaZpQEzg+0fkZQI\njAZ6Ay2BKyS1DL5+yMxam1kb4G3gnrCh68ysTfAaUGCfFwN7CxzqBmCXmTUHRgIPBH1rAvcCZwId\ngHuPFJLOlVY1q1bk4b5tebZ/Orv3fc/Fj8/h/neWs++AFyp1JVO04ZMJjA0+jwUujNCnA7DWzNab\n2QFgfDAOM9sT1q8qUOhvTSVVA7KAv/zEXCYCv5IkoCeQbWY7zWwXkA30KsLanCt1fnVKHaZnZdC3\nQyOenvUpPUfl8sG6L+M9Lef+n2jDp46ZbQk+bwXqROhTH9gYtr0paANA0v2SNgL9+PGZT5PgkluO\npC5h7cOAfwDfHuk4ZnYQ2A2cWNjxnStrqleuwF8vOo2XbzoLCa58+kP++PoS9nihUleCFBo+kmZI\nWhrhlRnez0L/4u2o7/c0s7vNrCEwDhgYNG8BGgWX47KAlyRVl9QGaGZmbxztcYpC0s2S8iTl5efn\nF8chnIuZjs1OZOrgDG7JaMor8zfSfUQOM5Z7oVJXMhQaPmbWzcxaRXhNArZJqgsQvG+PsIvNQMOw\n7QZBW0HjgEuCY+43sx3B5wXAOqAF0BFIl7QBmA20kPR+weNISgJqADuO4viY2VNmlm5m6SkpKT/1\nn8W5UuG4ion88bxTePP2TiRXqciNL+Rxx8sfs2Pv/nhPzZVz0V52mwwcvnusPzApQp/5QJqkJpIq\nAn2DcUhKC+uXCawM2lOCGxWQ1BRIA9ab2RNmVs/MUoHOwGozOyfCXC4F/h2cjU0DekhKDm406BG0\nOVdutG5wApMHdiarewumLg0VKp20aLOX6HFxE234DAe6S1oDdAu2kVRP0hT4z+9fBhL6gb8CmGBm\nyw6PDy7hLSEUCoOD9gxgiaRFhG4eGGBmOwuZy7PAiZLWErpUNzQ4/k5CvyeaH7zuK8K+nCtzKiYl\nMOhXabwzqAuNT6zK4PGLuGFsHl98tS/eU3PlkPxvPpGlp6dbXl5evKfhXLH44ZDx/Acb+Pu0VSQm\niKG9T+bKDo1ISFC8p+ZKOUkLzCy9sH5e4cC5cigxQdzQuQnT7szg9IY1+O83l3LF0/P49EsvVOpi\nw8PHuXKs0YlVePGGM3nwktYs37KHXqNyeTJnHQd/8BI9rnh5+DhXzkni8jMaMiOrKxktUvjbuyu5\n+IkPWLFlT+GDnfuZPHyccwDUqV6Zp65uz+gr2/HFV/v49aOzGTF9FfsPeoked+x5+Djn/kMS57eu\nS/aQrvQ5vR6P/HstFzwym4Wf74r31FwZ4+HjnPt/kqtWZMRv2vDP687gm/0HueSJD7jvreV8e+Bg\nvKfmyggPH+fcEZ17Um2mDcngqjMb89ycT+kxMpfZa7xQqYueh49z7icdX7kCwy5sxYRbOlIhMYGr\nnv2QP0xczO59XqjU/XwePs65IunQpCbvDu7Crec047WFm+k+Iodpy7bGe1qulPLwcc4VWeUKidzV\n62TevK0TJ1arxC3/WsDt4xaS/7UXKnVHx8PHOXfUTmtQg8kDO/H7nieRvXwb3Ufm8PrCTV6o1BWZ\nh49z7mepkJjA7ec2Z8rgzjRLqUbWhMVc+8/5bPZCpa4IPHycc1FpXvt4Xr2lI3/6dUvmb9hJjxE5\nvDB3A4cO+VmQOzIPH+dc1BISxLWdQoVK2zVO5p5Jy/jNU3NZl7833lNzJZSHj3PumGlYswovXN+B\nhy5tzaqtX9P74Vk8/v5aL1Tq/h8PH+fcMSWJy9IbMuO3XfnlSbV5cOoqLnx8Dsu+2B3vqbkSJKrw\nkVRTUrakNcF78hH69ZK0StJaSUPD2odJWiJpkaTpkuoF7amS9gXtiySNibDPyZKWhm1nSVoe7G+m\npMZh3/0Qtq/J0azZOVc0tY+vzJir2/NEv3Zs3b2fPo/N4aFpK/nuey9U6qI/8xkKzDSzNGBmsP0j\nkhKB0UBvoCVwhaSWwdcPmVlrM2sDvA3cEzZ0nZm1CV4DCuzzYqDgxeSPgXQza03o0dsPhn23L2xf\nfX72ap1zR633aXWZkZXBRW3rM/q9dZz3yCzyNviT7Mu7aMMnExgbfB4LXBihTwdgrZmtN7MDwPhg\nHGYW/sCQqkCht8dIqgZkAX8Jbzez98zs22BzHtDgKNbhnCtGJ1SpyN8vO50Xru/A/u8PcdmTc/nT\n5GV8s98LlZZX0YZPHTPbEnzeCtSJ0Kc+sDFse1PQBoCk+yVtBPrx4zOfJsFlshxJXcLahwH/AL7l\nyG4A3g3brixpoaR5kiIFpHMuBjJapDB9SAb9O6Yydu4GeozMJXd1fryn5eKg0PCRNEPS0givzPB+\nFvqnzUd9Y7+Z3W1mDYFxwMCgeQvQKLgclwW8JKm6pDZAMzN74yfmexWQDjwU1tzYzNoBVwKjJDU7\nwtibJeVJysvP9/9DOFccqlZK4k99TuXVWzpSqUIC1zz3Eb97dTFffXsg3lNzMVRo+JhZNzNrFeE1\nCdgmqS5A8L49wi42Aw3DthsEbQWNAy4JjrnfzHYEnxcA64AWQEcgXdIGYDbQQtL7h3cgqRtwN9DH\nzP5TbMqCiTlTAAAUFklEQVTMNgfv64H3gbZHWOtTZpZuZukpKSmF/JdxzkUjPbUmUwZ14fZzm/HG\nx5vpNiKXdz/ZUvhAVyZEe9ltMtA/+NwfmBShz3wgTVITSRWBvsE4JKWF9csEVgbtKcGNCkhqCqQB\n683sCTOrZ2apQGdgtZmdE/RrCzxJKHj+E4KSkiVVCj7XAjoBy6Nct3PuGKhcIZHf9zyZyQM7Uad6\nJW4dt5BbX1zA9q+/i/fUXDGLNnyGA90lrQG6BdtIqidpCoCZHSR0OW0asAKYYGbLDo8PLuEtAXoA\ng4P2DGCJpEWE7lwbYGaF3R7zEFANeLXALdWnAHmSFgPvAcPNzMPHuRLk1Ho1ePP2TtzV62RmrtxO\nt3/k8GreRi9UWobJ/3AjS09Pt7y8vHhPw7lyZ13+Xoa+toT5G3bRJa0Wf73oNBrWrBLvabkikrTA\nzNIL6+cVDpxzJUqzlGq8cnNHhmWeysLPdtFzVC7Pz/nUC5WWMR4+zrkSJyFBXN0xlWlDMjgjtSZ/\nems5lz05l7Xbv4731Nwx4uHjnCuxGiRX4fnrzmDE5aezLn8v5z08m9HvreV7L1Ra6nn4OOdKNElc\n3K4B2UO60v3UOjw0bRV9HpvD0s1eqLQ08/BxzpUKKcdXYvSV7Xjy6vZ8uXc/maPnMPxdL1RaWnn4\nOOdKlZ6n/oIZQ7pyabsGjMlZx3kPz+KjT71QaWnj4eOcK3VqVKnAA5e25sUbzuTAD4e4/Mm5/M+b\nS9nrhUpLDQ8f51yp1TmtFtOHZHB9pya8+OFn9BiRw3urIlX5ciWNh49zrlSrUjGJe37dkokDzqZK\npSSu++d8sl5ZxK5vvFBpSebh45wrE9o3TuadQZ0Z9MvmTF78Bd1G5PD2ki+8RE8J5eHjnCszKiUl\nktXjJN66ozP1TjiOgS99zC3/WsC2PV6otKTx8HHOlTmn1K3OG7edzR97n0zO6ny6jcjhlfmf+1lQ\nCeLh45wrk5ISE7ilazOm3pnBKXWrc9drn3DVsx/y+Y6fegiyixUPH+dcmdakVlXG33QWf7mwFYs3\n7qbnqFyenf0pP3ih0rjy8HHOlXkJCeKqsxozfUgGHZudyLC3l3PpmA9Ys80LlcaLh49zrtyod8Jx\nPNs/nYf7tmHDl99w3iOzeGTmGg4c9EKlsRZV+EiqKSlb0prgPfkI/XpJWiVpraShYe3DJC0Jnjw6\nXVK9oD1V0r6gfZGkMRH2OVnS0rDtayXlh425Mey7/sEc10jqX3BfzrnyQxKZbeozI6srvVrVZUT2\navo8NpvFG7+K99TKlWjPfIYCM80sDZgZbP+IpERgNNAbaAlcIall8PVDZtbazNoAbwP3hA1dZ2Zt\ngteAAvu8GNgbYT6vhI15JuhbE7gXOBPoANx7pJB0zpUfJ1arxKNXtOXpa9LZ9e0BLnp8Dn+bsoJ9\nB7xQaSxEGz6ZwNjg81jgwgh9OgBrzWy9mR0AxgfjMLM9Yf2qAoX+BlBSNSAL+EsR59gTyDaznWa2\nC8gGehVxrHOujOvesg7ZWV35zRkNeTJ3Pb0fzmXe+h3xnlaZF2341DGzLcHnrUCdCH3qAxvDtjcF\nbQBIul/SRqAfPz7zaRJcPsuR1CWsfRjwDyDS/ZKXSPpE0kRJDYty/HCSbpaUJykvPz8/UhfnXBlU\nvXIF/nZxa1668UwOGfR9ah53v/EJX3/3fbynVmYVGj6SZkhaGuGVGd7PQv9666jvXTSzu82sITAO\nGBg0bwEaBZfjsoCXJFWX1AZoZmZvRNjVW0CqmZ1G6OxmbIQ+hc3lKTNLN7P0lJSUox3unCvlzm5e\ni2l3ZnBTlya8/NHn9BiZy79Xbov3tMqkQsPHzLqZWasIr0nANkl1AYL3SOVkNwMNw7YbBG0FjQMu\nCY6538x2BJ8XAOuAFkBHIF3SBmA20ELS+0G/HWa2P9jXM0D7ozy+c85xXMVE7j6/Ja/f1onqlStw\n/fN5DB7/MTv27i98sCuyaC+7TQYO3z3WH5gUoc98IE1SE0kVgb7BOCSlhfXLBFYG7SnBjQpIagqk\nAevN7Akzq2dmqUBnYLWZnRP0qxu2rz7AiuDzNKCHpOTgRoMeQZtzzh1Rm4Yn8NYdnbmzWxpTPtlC\n95G5TF7shUqPlWjDZzjQXdIaoFuwjaR6kqYAmNlBQpfTphEKhAlmtuzw+OAS3hJCoTA4aM8Alkha\nBEwEBphZYY8qHCRpmaTFwCDg2uD4Own9nmh+8LqvCPtyzjkqJiVwZ7cWvH1HFxrWrMKglz/mphfy\n2LrbC5VGS57ikaWnp1teXl68p+GcKyF+OGT8c86n/H36KiokJPBf559C3zMaIineUytRJC0ws/TC\n+nmFA+ecK4LEBHFjl6ZMuzODVvVr8MfXP+HKpz/ksx3fxHtqpZKHj3POHYXGJ1blpZvOZPjFp7F0\nc6hQ6dO5671Q6VHy8HHOuaMkib4dGpGd1ZXOzWtx/5QVXPz4HFZt9UKlReXh45xzP9MvalTm6WvS\nefSKtmzatY8LHp3FyOzVXqi0CDx8nHMuCpL49en1yM7qyvmn1eXhmWu44NFZLPJCpT/Jw8c5546B\nmlUrMqpvW567Np2vvzvIxY/P4S9vL/dCpUfg4eOcc8fQL0+uw/QhGVzRoRHPzP6UnqNy+WDtl/Ge\nVonj4eOcc8fY8ZUrcP9FpzH+5rNIEFz5zIcMfW0Ju/d5odLDPHycc66YnNX0RKbemcEtXZsyIW8j\nPUbmkL3cC5WCh49zzhWryhUS+WPvU3jz9k4kV6nITS/kMfClhXxZzguVevg451wMtG5wApMHdua3\n3Vswfdk2uo/I4c2PN5fbQqUePs45FyMVkxK441dpvDOoM6m1qnLnK4u4YWweX3y1L95TizkPH+ec\ni7G0OsczccDZ3HNBS+au20GPkbm8OO8zDpWjEj0ePs45FweJCeL6zk2YPiSDNg1P4L/fXErfp+fx\n6Zflo1Cph49zzsVRw5pV+NcNHXjwktas2LKHXqNyGZOzjoM/lO0SPR4+zjkXZ5K4/IyGzMjqStcW\nKQx/dyUXPf4By7/YE++pFZuowkdSTUnZktYE78lH6NdL0ipJayUNDWsfJmmJpEWSpkuqF7SnStoX\ntC+SNCbCPidLWhq2PTKs/2pJX4V990PYd5OjWbNzzhWXOtUr8+TV7Rl9ZTu27N5Hn8dm84/pq9h/\nsOyV6InqSaaSHgR2mtnwIFSSzeyuAn0SgdVAd2AToUdZX2FmyyVVN7M9Qb9BQEszGyApFXjbzFod\n4bgXA5cCrSP1kXQH0NbMrg+295pZtaNZmz/J1DkXT7u+OcCwd5bz+sLNNK9djQcuaU37xhH/fl+i\nxOpJppnA2ODzWODCCH06AGvNbL2ZHQDGB+M4HDyBqkChSSipGpAF/OUnul0BvFzo7J1zroRKrlqR\nEZe34fnrzmDfgR+4dMwH/PmtZXyz/2C8p3ZMRBs+dcxsS/B5K1AnQp/6wMaw7U1BGwCS7pe0EegH\n3BPWr0lwmSxHUpew9mHAP4BvI01IUmOgCfDvsObKkhZKmicpUkAeHnuzpDxJefn5+Ufq5pxzMXPO\nSbWZNiSDq89qzD/nbKDnqFxmrSn9P58KDR9JMyQtjfDKDO9noet3R30Nz8zuNrOGwDhgYNC8BWhk\nZm0IneW8JKm6pDZAMzN74yd22ReYaGbhF0kbm1k74EpglKRmR5jLU2aWbmbpKSkpR7sU55wrFtUq\nJXFfZism3NKRiokJXP3sR/xh4mJ2f1t6C5UWGj5m1s3MWkV4TQK2SaoLELxvj7CLzUDDsO0GQVtB\n44BLgmPuN7MdwecFwDqgBdARSJe0AZgNtJD0foH99KXAJTcz2xy8rwfeB9oWtm7nnCtpOjSpyZTB\nXbj1nGa8tnAz3UbmMHXp1nhP62eJ9rLbZKB/8Lk/MClCn/lAmqQmkioSCofJAJLSwvplAiuD9pTg\nRgUkNQXSgPVm9oSZ1TOzVKAzsNrMzjm8A0knA8nA3LC2ZEmVgs+1gE7A8ijX7ZxzcVG5QiJ39TqZ\nSbd3IqVaJQa8uIDbxy0k/+vSVag02vAZDnSXtAboFmwjqZ6kKQBmdpDQ5bRpwApggpktOzw+uIS3\nBOgBDA7aM4AlkhYBE4EBZrazCPPpC4y3H9/CdwqQJ2kx8B4w3Mw8fJxzpVqr+jWYNLATv+95Etkr\nttFtRA6vLdhUagqVRnWrdVnmt1o750qLtdv3ctdrS1jw2S4yWqTw14ta0SC5SlzmEqtbrZ1zzsVZ\n89rVePWWjvy5z6nkbdhJz5G5vDB3Q4kuVOrh45xzZUBCguh/dirT7sygXeNk7pm0jN88NZd1+Xvj\nPbWIPHycc64MaVizCi9c34G/X3Y6q7ftpffDs3j8/bV8X8IKlXr4OOdcGSOJS9s3IDsrg26n1ObB\nqau4cPQclm7eHe+p/YeHj3POlVG1j6/M4/3aM+aqdmzbs5/M0XN4aNpKvvs+/oVKPXycc66M69Wq\nLjOzunJx2/qMfm8d5z0yi7wNRfnXK8XHw8c558qBGlUq8NBlp/PC9R3Y//0hLntyLvdOWsreOBUq\n9fBxzrlyJKNFCtOHZNC/YyovzPuMniNzyVkd+0KlHj7OOVfOVK2UxJ/6nMqrt3SkcoUE+j/3Eb+d\nsJivvj0Qszl4+DjnXDmVnlqTdwZ1YeC5zZm0aDPdRuTy7idbCh94DHj4OOdcOVa5QiK/63kSkwZ2\n4hc1KnHruIXcPm5hsVdHSCrWvTvnnCsVTq1Xgzdv68Qzsz9l73cHSUhQsR7Pw8c55xwASYkJDOga\n8Vmbx5xfdnPOORdzHj7OOediLqrwkVRTUrakNcF78hH69ZK0StJaSUPD2odJWiJpkaTpkuoF7amS\n9gXtiySNCRvzfrCvw9/VDtorSXolOMaHklLDxvQP5rhGUn+cc87FVbRnPkOBmWaWBswMtn8keBz2\naKA30BK4QlLL4OuHzKy1mbUB3gbuCRu6zszaBK8BBXbbL+y77UHbDcAuM2sOjAQeCI5fE7gXOBPo\nANx7pJB0zjkXG9GGTyYwNvg8FrgwQp8OwFozW29mB4DxwTjMbE9Yv6pANPf2hc9lIvArSQJ6Atlm\nttPMdgHZQK8ojuOccy5K0YZPHTM7/C+StgJ1IvSpD2wM294UtAEg6X5JG4F+/PjMp0lwWS1HUpcC\n+xwbfPc/QcD86DhmdhDYDZxY2PGdc87FXqHhI2mGpKURXpnh/czM+BlnLmZ2t5k1BMYBA4PmLUCj\n4HJcFvCSpOrBd/3M7FSgS/C6+miPeSSSbpaUJykvPz/2tY6cc668KDR8zKybmbWK8JoEbJNUFyB4\n3x5hF5uBhmHbDYK2gsYBlwTH3G9mO4LPC4B1QItge3Pw/jXwEqHLej86jqQkoAaw4yiOj5k9ZWbp\nZpaekpLyU/9ZnHPORSHaf2Q6GegPDA/eJ0XoMx9Ik9SE0A/9vsCVAJLSzGxN0C8TWBm0pwA7zewH\nSU2BNGB9EConmNmXkioAFwAzCsxlLnAp8G8zM0nTgL+G3WTQA/hjYQtbsGDBl5I+O4r/FgXVAr6M\nYnxpVN7WXN7WC77m8iKaNTcuSqdow2c4MEHSDcBnwOUAwS3Tz5jZeWZ2UNJAYBqQCDxnZssOj5d0\nEnAoGH/4rrYM4D5J3wffDTCznZKqAtOC4EkkFDxPB2OeBf4laS2wk1DIEYwbRigEAe4zs0KfomRm\nUZ36SMozs/Ro9lHalLc1l7f1gq+5vIjFmhX6VY071vx/sGVfeVsv+JrLi1is2SscOOecizkPn+Lz\nVLwnEAflbc3lbb3gay4vin3NftnNOedczPmZj3POuZjz8InCkQqmhn0vSY8E3y+R1C4e8zyWirDm\nfsFaP5H0gaTT4zHPY6mwNYf1O0PSQUmXxnJ+xaEoa5Z0TlBpZJmknFjP8Vgrwv+2a0h6S9LiYM3X\nxWOex4qk5yRtl7T0CN8X788vM/PXz3gRutV7HdAUqAgsBloW6HMe8C4g4Czgw3jPOwZrPhtIDj73\nLg9rDuv3b2AKcGm85x2DP+cTgOWEKpEA1I73vGOw5v8CHgg+pxD6Jx0V4z33KNacAbQDlh7h+2L9\n+eVnPj/fEQumhskEXrCQecAJhytClFKFrtnMPrBQAVeAeYQqSpRmRflzBrgDeI3IVT5Km6Ks+Urg\ndTP7HMD+r7p8aVWUNRtwfFBPshqh8DkY22keO2aWS2gNR1KsP788fH6+ohQsLWtFTY92PTcQ+ptT\naVbomiXVBy4CnojhvIpTUf6cWwDJCj1fa4Gka2I2u+JRlDU/BpwCfAF8Agw2s0OxmV5cFOvPr2gr\nHDgXkaRzCYVP53jPJQZGAXeZ2aH/K7Je5iUB7YFfAccBcyXNM7PV8Z1WseoJLAJ+CTQDsiXNsh8/\nGsYVkYfPz1eUgqVFLmpaShRpPZJaA88AvS0oEFuKFWXN6cD4IHhqAedJOmhmb8ZmisdcUda8Cdhh\nZt8A30jKBU4HSmv4FGXN1wHDLfQLkbWSPgVOBj6KzRRjrlh/fvllt5/vPwVTJVUkVEtucoE+k4Fr\ngrtGzgJ22/89/6g0KnTNkhoBrwNXl5G/BRe6ZjNrYmapZpZK6EGGt5Xi4IGi/W97EtBZUpKkKoSe\nFLwixvM8loqy5s8JnekhqQ5wErA+prOMrWL9+eVnPj+THaFgqqQBwfdjCN35dB6wFviW0N+cSq0i\nrvkeQg/xezw4EzhopbguVhHXXKYUZc1mtkLSVGAJoeK/z5hZxFt2S4Mi/jkPA56X9AmhO8DuMrNS\nW+1a0svAOUAtSZuAe4EKEJufX17hwDnnXMz5ZTfnnHMx5+HjnHMu5jx8nHPOxZyHj3POuZjz8HHO\nORdzHj7OOedizsPHOedczHn4OOeci7n/BcLW0G1HQd7vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10df10250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.67432485e-02,  -1.58193604e-03,  -1.47534160e-04,\n",
       "         9.77362451e-03,   1.23372063e-02,   7.05223551e-05,\n",
       "        -1.14090796e-03,  -1.92714643e-07,  -3.33120094e-02,\n",
       "        -6.12246659e-04])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space.low )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_policy_action(env.env.state, w_est, bfs, env, method=\"continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 143 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 91 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 95 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 143 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 98 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 94 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 96 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 73 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 102 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 143 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 98 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 143 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 91 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 95 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 132 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 94 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 91 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 74 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 102 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 98 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 132 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 91 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 102 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 96 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 96 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 98 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 73 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 96 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 95 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 94 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 98 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 132 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 98 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 102 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 95 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 94 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 132 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 143 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 96 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 134 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 134 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 134 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 98 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 92 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 134 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 68 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 95 timesteps\n",
      "--------\n",
      "reward: 99.9\n",
      "Episode finished after 100 timesteps\n",
      "--------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ecda3af72bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_policy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#action = env.action_space.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/gym/envs/classic_control/continuous_mountain_car.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mglBegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGL_LINE_LOOP\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mGL_LINE_STRIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# draw each vertex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mglEnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_linewidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/pyglet/gl/lib.pyc\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "method = \"continuous\"\n",
    "num_steps = []\n",
    "for i_episode in range(100):\n",
    "    observation = env.reset()\n",
    "    print \"--------\"\n",
    "    t = 0\n",
    "    actions = []\n",
    "    while True:\n",
    "        t+=1\n",
    "        env.render()\n",
    "        action = get_policy_action(env.env.state, w_est, bfs, env, method = method)\n",
    "        #action = env.action_space.sample()\n",
    "        #print type(action)\n",
    "        actions.append([action])\n",
    "        if method == \"continuous\":\n",
    "            action = [action]\n",
    "        #print action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #print observation\n",
    "        if done:\n",
    "            print \"reward:\",reward\n",
    "            num_steps.append(t)\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [-1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(num_steps)\n",
    "print np.mean(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(num_steps)\n",
    "print np.mean(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# custom pendulum simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
