{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook for testing my implementation of LSPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import LSPI\n",
    "import basisFunctions\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes \n",
    "for cartpole sampling, use:\n",
    "1000 trials with 6 time steps\n",
    "resampling at each iteration seems to have no large impact on the problem  \n",
    "gamma = 0.95  \n",
    "epsilon = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04621836 -0.04659054 -0.0383234  -0.04201802]\n",
      "19.999112705753245\n",
      "19.98391494221708\n",
      "19.993363232950685\n",
      "[ 1.99907300e+01 -1.55294626e-03  1.29227463e-03  5.20710970e-04]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "bfs = basisFunctions.get_cartpole_basis_functions_quadratic_v2()\n",
    "\n",
    "gamma = 0.95\n",
    "epsilon = 0.01\n",
    "k = len(bfs) # the number of basis functions will be the size of parameter vector w\n",
    "\n",
    "w = np.zeros(k)\n",
    "w_est, w0 = LSPI.LSPI(bfs, gamma, epsilon, w, env, method = \"discrete\", n_trial_samples=1000, n_timestep_samples=6)\n",
    "print (w_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_est = [20, 0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notes: convergence is weird for get_mt_car_basis_functions_quadratic_v1,\n",
    "the result is oscillating:\n",
    "0.02229830449342507\n",
    "-1.975657115388799\n",
    "-1.9079250600368027\n",
    "-1.9329972236620279\n",
    "-1.9143671003801614\n",
    "-1.9192840196773204\n",
    "-1.9166902435441102\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.52570525  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackduryea/Desktop/RiceS19/COMP650/LSPI/LSPI.py:253: OptimizeWarning: Unknown solver options: xtol\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02229830449342507\n",
      "-1.975657115388799\n",
      "-1.9079250600368027\n",
      "-1.9329972236620279\n",
      "-1.9143671003801614\n",
      "-1.9192840196773204\n",
      "-1.9166902435441102\n",
      "[-1.92010449  0.39804835 -0.24925825 -0.24869455]\n"
     ]
    }
   ],
   "source": [
    "# convergence i \n",
    "bfs = basisFunctions.get_mt_car_basis_functions_quadratic_v1()\n",
    "\n",
    "gamma = 0.95\n",
    "epsilon = 0.05\n",
    "k = len(bfs) # the number of basis functions will be the size of parameter vector w\n",
    "\n",
    "w = np.zeros(k)\n",
    "w_est, w0 = LSPI.LSPI(bfs, gamma, epsilon, w, env, method = \"continuous\", n_trial_samples=1000, n_timestep_samples=20)\n",
    "print (w_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobat Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99581231  0.09142119  0.99626057  0.08639957  0.07723624 -0.09899189]\n",
      "-19.999704045169622\n",
      "[ 0.99730579  0.07335645  0.99526066  0.09724309  0.02434384 -0.07645477]\n",
      "[-1.99999109e+01 -6.00120670e-06 -2.85661150e-06  3.03274839e-09]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bfs = basisFunctions.get_acrobat_basis_functions_quadratic_v1()\n",
    "\n",
    "gamma = 0.95\n",
    "epsilon = 0.05\n",
    "k = len(bfs) # the number of basis functions will be the size of parameter vector w\n",
    "\n",
    "w = np.zeros(k)\n",
    "w_est, w0 = LSPI.LSPI(bfs, gamma, epsilon, w, env, method = \"discrete\", n_trial_samples=10000, n_timestep_samples=20)\n",
    "print (w_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 12 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 12 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 12 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 12 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 12 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 9 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 11 timesteps\n",
      "--------\n",
      "reward: 1.0\n",
      "Episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "env._max_episode_steps = 100000\n",
    "\n",
    "method = \"discrete\"\n",
    "#method = \"continuous\"\n",
    "num_steps = []\n",
    "for i_episode in range(100):\n",
    "    observation = env.reset()\n",
    "    print (\"--------\")\n",
    "    t = 0\n",
    "    actions = []\n",
    "    while True:\n",
    "        t+=1\n",
    "        env.render()\n",
    "        action = LSPI.get_policy_action(env.env.state, w_est, bfs, env, method = method)\n",
    "        # action = env.action_space.sample()\n",
    "        if method == \"continuous\":\n",
    "            action = [action[0]]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #print observation\n",
    "        if done:\n",
    "            print (\"reward:\",reward)\n",
    "            num_steps.append(t)\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.26\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(num_steps);\n",
    "print(np.mean(num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bfs = basisFunctions.get_cartpole_basis_functions_quadratic_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfs[0](np.array([1,2,3,2]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53597464,  0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1., -1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1., -1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((7,7))-2*np.identity(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
