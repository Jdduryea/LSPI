{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rosgym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f642fd662bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrosgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rosgym'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import rosgym\n",
    "import sys\n",
    "import imp\n",
    "import getpass\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import rospy\n",
    "import imp\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + '/python')\n",
    "\n",
    "import basic_agents\n",
    "import actor_critic\n",
    "import ddpg_agent\n",
    "import mp_exploring_from_experience_config_ddpg\n",
    "\n",
    "from std_msgs.msg import String\n",
    "from geometry_msgs.msg import PointStamped\n",
    "from visualization_msgs.msg import Marker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"/home/cannon/reinforcement_learning/rl_agents/config/fast_config.yaml\"\n",
    "DEFAULT_CONFIG_FILE = \"/home/cannon/reinforcement_learning/rl_agents/config/default_config.yaml\"\n",
    "\n",
    "def get_key_or_default(vals, defaults, key):\n",
    "    try:\n",
    "        ret_thing = vals[key]\n",
    "    except (KeyError, TypeError):\n",
    "        ret_thing = defaults[key]\n",
    "    return ret_thing\n",
    "\n",
    "def get_configs(vals, defaults):\n",
    "    agent_config = get_key_or_default(vals, defaults, 'agent')\n",
    "    exp_config = get_key_or_default(vals, defaults, 'experiment')\n",
    "    return agent_config, exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(CONFIG_FILE) as yaml_file:\n",
    "    config = yaml.load(yaml_file)\n",
    "    \n",
    "with open(DEFAULT_CONFIG_FILE) as yaml_file:\n",
    "    defaults = yaml.load(yaml_file)\n",
    "    \n",
    "def retrieve(key):\n",
    "    return get_key_or_default(agent_config, defaults['agent'], key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config, exp_config = get_configs(config, defaults)\n",
    "\n",
    "SLACK_WEBHOOK = get_key_or_default(exp_config, defaults['experiment'], 'slack_webhook')\n",
    "EXP_NAME = get_key_or_default(exp_config, defaults['experiment'], 'name')\n",
    "COMPUTER_NAME = get_key_or_default(exp_config, defaults['experiment'], 'computer_name')\n",
    "\n",
    "time_str = str(time.time())[:10]\n",
    "REWARD_FILE = '/home/cannon/reinforcement_learning/experiments/' + COMPUTER_NAME + \\\n",
    "                '_reward_history_' + EXP_NAME + '_' + time_str + '.txt'\n",
    "TIME_FILE = '/home/cannon/reinforcement_learning/experiments/' + COMPUTER_NAME + \\\n",
    "                '_time_history_' + EXP_NAME + '_' + time_str + '.txt'\n",
    "TEST_FILE = '/home/cannon/reinforcement_learning/experiments/' + COMPUTER_NAME + \\\n",
    "                '_test_reward_history_' + EXP_NAME + '_' + time_str + '.txt'\n",
    "TEST_SUCCESS_FILE = '/home/cannon/reinforcement_learning/experiments/' + COMPUTER_NAME + \\\n",
    "                '_test_success_history_' + EXP_NAME + '_' + time_str + '.txt'\n",
    "\n",
    "if os.path.isfile(REWARD_FILE):\n",
    "    os.remove(REWARD_FILE)\n",
    "    \n",
    "if os.path.isfile(TIME_FILE):\n",
    "    os.remove(TIME_FILE)\n",
    "    \n",
    "if os.path.isfile(TEST_FILE):\n",
    "    os.remove(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this variable to the path to fetch_config.py on your local machine.\n",
    "my_fetch_config = \"/home/\" + getpass.getuser() + \"/rl_wksp/src/rosgym/src/rosgym/ur5_config_random_goal.py\"\n",
    "env = rosgym.make_randomgoal_robot_env(\"ur5_config_random_goal\", my_fetch_config, retrieve('planning_group'),\n",
    "                                      retrieve('num_joints'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env.reset()\n",
    "print env.observation_space.sample()\n",
    "env.action_space.sample()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_pub = rospy.Publisher('visualization_marker', Marker, queue_size=10)\n",
    "robot_marker = Marker()\n",
    "\n",
    "state = PointStamped()\n",
    "\n",
    "state.point.x = 1\n",
    "state.point.y = 1\n",
    "state.point.z = 1\n",
    "\n",
    "robot_marker = Marker()\n",
    "robot_marker.header.frame_id = \"/base_link\"\n",
    "robot_marker.header.stamp = rospy.get_rostime()\n",
    "robot_marker.ns = \"basic_shapes\"\n",
    "robot_marker.id = 1#!/usr/bin/env python\n",
    "\n",
    "\"\"\" File containing ExplorationPolicy and some implementations using motion planning. \"\"\"\n",
    "\n",
    "import abc\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import ast\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import actionlib\n",
    "import rospy\n",
    "\n",
    "from moveit_msgs.srv import GetPositionIK\n",
    "from moveit_msgs.msg import MoveItErrorCodes, PositionIKRequest\n",
    "from moveit_python import MoveGroupInterface, PlanningSceneInterface\n",
    "from control_msgs.msg import FollowJointTrajectoryGoal, FollowJointTrajectoryAction\n",
    "from actionlib_msgs.msg import GoalStatus\n",
    "from geometry_msgs.msg import PoseStamped\n",
    "\n",
    "from tf import TransformListener\n",
    "\n",
    "from std_srvs.srv import Empty\n",
    "\n",
    "\n",
    "class ExplorationPolicy(object):\n",
    "    \"\"\" Abstract base class of exploration policy hierarchy. \"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def connect(self):\n",
    "        \"\"\" \n",
    "        Method to allow the exploration policy to do any configuration necessary for internals.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def explore(self, guidance):\n",
    "        \"\"\"\n",
    "        Method to explore via the exploration policy, and return some record of the exploration.\n",
    "        :param guidance Parameter specifying any guidance given by the learning algorithm to the exploration policy, \n",
    "        i.e. for active learning.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "FEEDBACK_FILE = \"/home/cannon/reinforcement_learning/feedback.txt\"\n",
    "robot_marker.type = Marker.TEXT_VIEW_FACING\n",
    "robot_marker.action = Marker.ADD\n",
    "robot_marker.pose.position = state.point\n",
    "robot_marker.pose.orientation.x = 0\n",
    "robot_marker.pose.orientation.y = 0\n",
    "robot_marker.pose.orientation.z = 0\n",
    "robot_marker.pose.orientation.w = 1.0\n",
    "\n",
    "robot_marker.color.r = 1.0\n",
    "robot_marker.color.g = 1.0\n",
    "robot_marker.color.b = 1.0\n",
    "robot_marker.color.a = 1.0\n",
    "\n",
    "robot_marker.scale.z = 0.1\n",
    "robot_marker.text = \"Not started\"\n",
    "\n",
    "robot_marker.lifetime = rospy.Duration(1)\n",
    "\n",
    "def timer_callback(event):\n",
    "    marker_pub.publish(robot_marker)\n",
    "\n",
    "timer = rospy.Timer(rospy.Duration(0.01), timer_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = mp_exploring_from_experience_config_ddpg.DDPGAgent(env, \n",
    "                                                            retrieve('discount_factor'), \n",
    "                                                            buffer_size=retrieve('buffer_size'),\n",
    "                                                            batch_size=retrieve('batch_size'),\n",
    "                                                            num_motion_planned=retrieve('num_motion_planned'),\n",
    "                                                            num_demonstrations=retrieve('num_demonstrations'),\n",
    "                                                            exploration_rate=retrieve('exploration_rate'),\n",
    "                                                            episode_length=get_key_or_default(exp_config, defaults['experiment'], 'episode_length'),\n",
    "                                                            actor_learning_rate=retrieve('actor_learning_rate'),\n",
    "                                                            critic_learning_rate=retrieve('critic_learning_rate'),\n",
    "                                                            tau=retrieve('tau'), \n",
    "                                                            use_random_goal=retrieve('use_random_goal'),\n",
    "                                                            critic_hidden_layers=retrieve('critic_hidden_layers'),\n",
    "                                                            actor_hidden_layers=retrieve('actor_hidden_layers'),\n",
    "                                                            planning_group=retrieve('planning_group'),\n",
    "                                                            restore=False,\n",
    "                                                            save_loc=\"./models/model.ckpt\",\n",
    "                                                            prioritized_replay=False)\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "\n",
    "with open(REWARD_FILE, 'w') as out_file, open(TIME_FILE, 'w') as time_file, open(TEST_FILE, 'w') as test_file, open(TEST_SUCCESS_FILE, 'w') as test_success_file:\n",
    "    # Main training loop\n",
    "    for i in xrange(get_key_or_default(exp_config, defaults['experiment'], 'num_episodes')):\n",
    "        \n",
    "        # Periodically run test episodes with deterministic runs, average and report.\n",
    "        if i % get_key_or_default(exp_config, defaults['experiment'], 'test_frequency') == 0:\n",
    "            agent.save()\n",
    "            test_success = []\n",
    "            for test in range(get_key_or_default(exp_config, defaults['experiment'], 'num_tests')): \n",
    "                state = agent.reset()\n",
    "                test_rewards = []\n",
    "                success = False\n",
    "                for j in xrange(get_key_or_default(exp_config, defaults['experiment'], 'episode_length')):\n",
    "#                     try:\n",
    "                    state, action, reward, next_state, done = agent.step(i, j, testing=True)\n",
    "                    if reward > 0:\n",
    "                        success = True\n",
    "#                     except Exception as err:\n",
    "#                         text = \"Exception occurred at testing step {} of episode {} on computer {}\".format(j, i, COMPUTER_NAME)\n",
    "#                         print err\n",
    "#                         requests.post(SLACK_WEBHOOK, json={'text': text})\n",
    "                    \n",
    "#                     state, action, reward, next_state, done = agent.step(i, j)\n",
    "                    test_rewards.append(reward)\n",
    "    \n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                test_success_file.write(str(1 if success else 0) + '\\n')\n",
    "                test_file.write(str(np.sum(test_rewards)) + '\\n')\n",
    "            \n",
    "            test_success_file.write('===\\n')\n",
    "            test_success_file.flush()\n",
    "            test_file.write('===\\n')\n",
    "            test_file.flush()\n",
    "        \n",
    "        \n",
    "        # Reset the environment before each new episode\n",
    "        ep_start = time.time()\n",
    "        state = agent.reset()\n",
    "        step_rewards = []\n",
    "                    \n",
    "        for j in xrange(get_key_or_default(exp_config, defaults['experiment'], 'episode_length')):\n",
    "#             Take a step, recording statistics\n",
    "#             try:\n",
    "            state, action, reward, next_state, done = agent.step(i, j)\n",
    "#             except Exception as err:\n",
    "#                 text = \"Exception occurred at step {} of episode {} on computer {}\".format(j, i, COMPUTER_NAME)\n",
    "#                 print \"ERROR: \" + str(err)\n",
    "#                 requests.post(SLACK_WEBHOOK, json={'text': text})\n",
    "                \n",
    "#             state, action, reward, next_state, done = agent.step(i, j)\n",
    "\n",
    "\n",
    "            robot_marker.text = \"Episode {}, Step {}\\nReward: {}\\nAction: {}\\nState: \\n\\tPos: {} \\n\\tVel: {}\\n\\tRel Dist: {}\\n\\tGoal: {}\".format(i, j, reward, action, state[:3], state[3:6], state[-6:-3], state[-3:])\n",
    "        \n",
    "            step_rewards.append(reward)\n",
    "            out_file.write(str(reward) + \"\\n\")\n",
    "            \n",
    "            # For tailing the reward file.\n",
    "            if j % 10 == 0:\n",
    "                out_file.flush()\n",
    "\n",
    "            # If the episode is over, go on to the next episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(np.sum(step_rewards))\n",
    "        out_file.write('---\\n')\n",
    "        out_file.flush()\n",
    "        \n",
    "        print \"\\nEpisode {} completed in {} seconds\\n\".format(i, str(time.time() - ep_start))\n",
    "        time_file.write(str(time.time() - ep_start) + \"\\n\")\n",
    "        time_file.flush()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            text = \"Cumulative rewards for the past 100 episodes on computer {} have been {}\".format(COMPUTER_NAME,\n",
    "                                                                                                     str(episode_rewards[-100:]))\n",
    "#             requests.post(SLACK_WEBHOOK, json={'text': text})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
